{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "import datetime\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load files\n",
    "sales_train = pd.read_csv(\"sales_train.csv\")\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "shops = pd.read_csv(\"shops.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "items = pd.read_csv(\"items.csv\")\n",
    "item_cat = pd.read_csv(\"item_categories.csv\")\n",
    "\n",
    "\n",
    "GROUPED_TRANSACTION_DATA_FILE = \"grouped_transaction_data.csv\"\n",
    "TRAINING_DATA_FILEPATH = \"training_data.csv\"\n",
    "training_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def YearFromDate(dateStr):\n",
    "    return dateStr.split('.')[2]\n",
    "    \n",
    "#add features\n",
    "if (path.exists(\"data.csv\")):\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "else:  \n",
    "    df = sales_train\n",
    "    # month is de maand, dus jan, feb etc.\n",
    "    df['month'] = df.apply(lambda r: r['date_block_num']%12, axis=1)\n",
    "    df['year'] = df.apply(lambda d: YearFromDate(d['date']), axis = 1)\n",
    "    df = df.drop('item_price', axis = 1) # can be dropped because item_price is just a function of shop_id, month and item_id\n",
    "    df = df.drop('date_block_num', axis = 1) # can be dropped since we have the year and month\n",
    "    df = df.drop('date', axis = 1 ) # can be dropped since we have year and month, don't need day\n",
    "    df.to_csv(\"data.csv\")\n",
    "\n",
    "group_filter = ['shop_id', 'item_id', 'year','month','item_cnt_day']\n",
    "#completing cleaning\n",
    "\n",
    "\n",
    "# sum the transaction in the same month and save the data\n",
    "if (path.exists(TRAINING_DATA_FILEPATH)):\n",
    "    training_data = pd.read_csv(TRAINING_DATA_FILEPATH)[group_filter]\n",
    "else:    \n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    summed_data = df.groupby(['shop_id', 'item_id', 'year','month']).sum().reset_index()\n",
    "    summed_data.to_csv(TRAINING_DATA_FILEPATH)\n",
    "    training_data = summed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the items that weren't sold in training month, but were sold in test month\n",
    "def addMissingTransactions():\n",
    "    for shop in tqdm(X_2['shop_id'].unique()):\n",
    "        for item in X_2[X_2['shop_id'] == shop]['item_id'].unique():\n",
    "            sold_next_month = X_2[(X_2['shop_id'] == shop) & \n",
    "                                  (X_2['item_id'] == item)]\n",
    "            sold_this_month = X_1[(X_1['shop_id'] == shop) & \n",
    "                                  (X_1['item_id'] == item)]\n",
    "            #if items not in training month but in test month\n",
    "            if len(sold_next_month) > 1 and len(sold_this_month) == 0:\n",
    "                print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "                closest_month = FindClosestMonth(next_month, shop, item)\n",
    "                print(closest_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "month = 9\n",
    "next_month = (month+1) % 12\n",
    "features = ['shop_id', 'item_id', 'year', 'month']\n",
    "years = training_data['year'].unique()\n",
    "\n",
    "X_1 = training_data[training_data['month'] == month][group_filter]\n",
    "X_train = X_1[(X_1['month'] == month) & (X_1['year'] < max(years))][features]\n",
    "y_train = X_1[(X_1['month'] == month) & (X_1['year'] < max(years))]['item_cnt_day']\n",
    "\n",
    "X_2 = training_data[training_data['month'] == month][group_filter]\n",
    "X_test = X_2[(X_2['month'] == month) & (X_2['year'] == max(years))][features]\n",
    "y_test = X_2[(X_2['month'] == month) & (X_2['year'] == max(years))]['item_cnt_day']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93685\n",
      "31531\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_shops = X_train['shop_id'].unique()\n",
    "X_items = X_train['item_id'].unique()\n",
    "X_months = np.array([i for i in range(0,12)])\n",
    "\n",
    "shop_labelEncoder = preprocessing.LabelEncoder().fit(X_shops)\n",
    "item_labelEncoder = preprocessing.LabelEncoder().fit(X_items)\n",
    "month_labelEncoder = preprocessing.LabelEncoder().fit(X_months)\n",
    "\n",
    "shop_labels = dict(zip(X_shops, shop_labelEncoder.transform(X_shops)))\n",
    "item_labels = dict(zip(X_items, item_labelEncoder.transform(X_items)))\n",
    "month_labels = dict(zip(X_months, month_labelEncoder.transform(X_months)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11736\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def vectorize(X):\n",
    "    matrix = np.zeros((len(X), len(X_shops) + len(X_items) + len(X_months) + 3), dtype=np.float32)\n",
    "    i = 0\n",
    "    for n, row in X.iterrows():\n",
    "        matrix[i][shop_labels[row['shop_id']]] = 1\n",
    "        matrix[i][len(shop_labels) + item_labels[row['item_id']]] = 1\n",
    "        matrix[i][len(shop_labels) + len(item_labels)] = row['year']\n",
    "        matrix[i][len(shop_labels) + len(item_labels)+ 3 + month_labels[row['month']]] = 1 # TODO: if i train for 1 month i can remove this\n",
    "        i+=1\n",
    "    return matrix\n",
    "        \n",
    "# matrix = vectorize(pd.DataFrame(X_split[0], columns = ['shop_id', 'item_id', 'year','month']))\n",
    "matrix = vectorize(X_train)\n",
    "print(len(matrix[0]))\n",
    "print(y_train.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "11736\n"
     ]
    }
   ],
   "source": [
    "print(type(matrix))\n",
    "print(type(y_train))\n",
    "print(len(matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3ec0c437cfb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# TODO: should change activation to relu after standardizing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DM Project\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m                \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                **kwargs):\n\u001b[1;32m-> 1143\u001b[1;33m     super(Dense, self).__init__(\n\u001b[0m\u001b[0;32m   1144\u001b[0m         activity_regularizer=activity_regularizer, **kwargs)\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DM Project\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\DM Project\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[0mbatch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_input_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_shape=(len(matrix[0],))),\n",
    "    Dense(1, )\n",
    "])\n",
    "# TODO: should change activation to relu after standardizing data\n",
    "model.add(Dense(1, activation='tanh')) # output layer\n",
    "optimizer = RMSprop(lr=0.005)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer = optimizer)\n",
    "model.fit(matrix, np.array(y_train), batch_size=128, epochs = 13)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
